{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvpAmw1VzuBo"
   },
   "source": [
    "# Text classification with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT0agDlhhh38"
   },
   "source": [
    "## Table of contents\n",
    "- [Imports](#im)\n",
    "- [1: Data and baseline](#1)\n",
    "- [2: Preparing the data for LSTMs](#2) \n",
    "- [3: Defining LSTM network architecture](#3) \n",
    "- [4: Training and evaluation](#4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPL5PqVKhh3-"
   },
   "source": [
    "### Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TVFXo-LUJwwU"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OgJkEb7hh3_"
   },
   "source": [
    "## Introduction \n",
    "<hr>\n",
    "\n",
    "Variants of recurrent neural networks such as long short-term memory networks (LSTMs) are one of the most popular models in natural language processing. But training them using deep learning libraries such as `PyTorch` is quite an involved process. In this lab, my intention is to familiarize you with the steps involved in training LSTMs for text classification using `PyTorch` and `torchtext`. Since we have not used `torchtext` much before, this lab has a tutorial-like format, where most of the code is given to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fK6qnuwDhh4A",
    "outputId": "0c11357c-8cb4-4498-c834-9d42f789d5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# You will need to upload your downloaded datafile to Google drive and mount the drive as follows.\n",
    "# Read files from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ljCRjchkJ12",
    "outputId": "d2736fc3-ae82-421d-dc52-a7efe597229d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_hm.csv\ttest.csv  train.csv  valid.csv\n"
     ]
    }
   ],
   "source": [
    "!ls \"/content/gdrive/MyDrive/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3XYmHk_k4MH",
    "outputId": "5a61accb-0b4c-40d5-84dc-5aa574dfe738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DpexJCwiPp1"
   },
   "outputs": [],
   "source": [
    "#pd.read_csv('gdrive/MyDrive/data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAX2bjZhh4A"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "no13KkGnhh4A"
   },
   "source": [
    "## 1: Data and baseline <a name=\"1\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfEUUiIKhh4A"
   },
   "source": [
    "We will use [HappyDB](https://www.kaggle.com/ritresearch/happydb) corpus which contains about 100,000 happy moments classified into 7 categories: *affection, exercise, bonding, nature, leisure, achievement, enjoy_the_moment*. The data was crowd-sourced via [Amazon Mechanical Turk](https://www.mturk.com/). The ground truth label is not available for all examples, and in this lab, we'll only use the examples where ground truth is available. \n",
    "\n",
    "**To do:**\n",
    "- Download the data from [here](https://www.kaggle.com/ritresearch/happydb).\n",
    "- Unzip the file and copy it in the lab directory.\n",
    "- Create a folder called `data` in your lab directory. \n",
    "\n",
    "For `torchtext`, we need train, valid, and test CSVs in a folder. Below I am providing some starter code to \n",
    "\n",
    "- read the data CSV (assuming it's saved as `cleaned_hm.csv` in your current directory)\n",
    "- split the data into train, valid, and test portions\n",
    "- write `train.csv`, `valid.csv`, and `test.csv` files in the `data` directory you have created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "OgVtq-QShh4B",
    "outputId": "40f09b8a-0324-4761-eac0-39203b1ad025"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wid</th>\n",
       "      <th>reflection_period</th>\n",
       "      <th>original_hm</th>\n",
       "      <th>cleaned_hm</th>\n",
       "      <th>modified</th>\n",
       "      <th>num_sentence</th>\n",
       "      <th>ground_truth_category</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hmid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>2053</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went on a successful date with someone I felt sympathy and connection with.</td>\n",
       "      <td>I went on a successful date with someone I felt sympathy and connection with.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>2</td>\n",
       "      <td>24h</td>\n",
       "      <td>I was happy when my son got 90% marks in his examination</td>\n",
       "      <td>I was happy when my son got 90% marks in his examination</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>1936</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>I went to the gym this morning and did yoga.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>206</td>\n",
       "      <td>24h</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>We had a serious talk with some friends of ours who have been flaky lately. They understood and we had a good evening hanging out.</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>6227</td>\n",
       "      <td>24h</td>\n",
       "      <td>I went with grandchildren to butterfly display at Crohn Conservatory\\r\\n</td>\n",
       "      <td>I went with grandchildren to butterfly display at Crohn Conservatory\\r\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>45</td>\n",
       "      <td>24h</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>I meditated last night.</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>leisure</td>\n",
       "      <td>leisure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>195</td>\n",
       "      <td>24h</td>\n",
       "      <td>I made a new recipe for peasant bread, and it came out spectacular!</td>\n",
       "      <td>I made a new recipe for peasant bread, and it came out spectacular!</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>achievement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27680</th>\n",
       "      <td>740</td>\n",
       "      <td>24h</td>\n",
       "      <td>I got gift from my elder brother which was really surprising me</td>\n",
       "      <td>I got gift from my elder brother which was really surprising me</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27681</th>\n",
       "      <td>3</td>\n",
       "      <td>24h</td>\n",
       "      <td>YESTERDAY MY MOMS BIRTHDAY SO I ENJOYED</td>\n",
       "      <td>YESTERDAY MY MOMS BIRTHDAY SO I ENJOYED</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>enjoy_the_moment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27682</th>\n",
       "      <td>4833</td>\n",
       "      <td>24h</td>\n",
       "      <td>Watching cupcake wars with my three teen children</td>\n",
       "      <td>Watching cupcake wars with my three teen children</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        wid reflection_period  ... ground_truth_category predicted_category\n",
       "hmid                           ...                                         \n",
       "27673  2053  24h               ...  NaN                   affection        \n",
       "27674  2     24h               ...  NaN                   affection        \n",
       "27675  1936  24h               ...  NaN                   exercise         \n",
       "27676  206   24h               ...  bonding               bonding          \n",
       "27677  6227  24h               ...  NaN                   affection        \n",
       "27678  45    24h               ...  leisure               leisure          \n",
       "27679  195   24h               ...  NaN                   achievement      \n",
       "27680  740   24h               ...  NaN                   affection        \n",
       "27681  3     24h               ...  NaN                   enjoy_the_moment \n",
       "27682  4833  24h               ...  NaN                   affection        \n",
       "\n",
       "[10 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"cleaned_hm.csv\", index_col=0)\n",
    "df = pd.read_csv('gdrive/MyDrive/data/cleaned_hm.csv',index_col=0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPQqlmsOhh4B",
    "outputId": "06b8e30c-26cd-40ca-d6d7-a7195b2e5d54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affection           4810\n",
       "achievement         4276\n",
       "bonding             1750\n",
       "enjoy_the_moment    1514\n",
       "leisure             1306\n",
       "nature              252 \n",
       "exercise            217 \n",
       "Name: ground_truth_category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ground_truth_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUzQt0ehhh4B",
    "outputId": "9a2427f6-ddbf-4fed-d5d0-584d080f8d85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14125, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = df.dropna()\n",
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4U1sFFFhh4C",
    "outputId": "80b6c720-ecbf-445f-caa5-3f13aedb8a01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affection           4810\n",
       "achievement         4276\n",
       "bonding             1750\n",
       "enjoy_the_moment    1514\n",
       "leisure             1306\n",
       "nature              252 \n",
       "exercise            217 \n",
       "Name: ground_truth_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"ground_truth_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_fZtAzahh4C",
    "outputId": "476fd0e4-856f-4b1d-d5e8-83081613a2d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "sample_df.rename(\n",
    "    columns={\"cleaned_hm\": \"moment\", \"ground_truth_category\": \"label\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ab5F-Vcfhh4C"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cLuhqS_phh4C"
   },
   "outputs": [],
   "source": [
    "big_train_df, test_df = train_test_split(sample_df, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VCQdUPmXhh4D"
   },
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(big_train_df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-GzjMDpqhh4D"
   },
   "outputs": [],
   "source": [
    "cols = [\"moment\", \"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1pO1nKS_hh4D"
   },
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"data/train.csv\", columns=cols, index=False)\n",
    "#valid_df.to_csv(\"data/valid.csv\", columns=cols, index=False)\n",
    "#test_df.to_csv(\"data/test.csv\", columns=cols, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOHwJt33hh4D"
   },
   "source": [
    "You should now have `train.csv`, `valid.csv`, and `test.csv` files written under the `data` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dtA2civjhh4D"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaZzsBl0hh4D"
   },
   "source": [
    "### 1.1 Baselines \n",
    "\n",
    "So far we have been carrying out text classification with bag-of-words (`CountVectorizer`) features and traditional machine learning models such as logistic regression and SVMs, which are not capable of dealing with sequential data. Bag-of-words representation does not take ordering of the words into account. For instance, with bag-of-words representation, S1 and S2 below would have the same interpretation. \n",
    "\n",
    "> S1: The movie was not bad. It was good. \n",
    "\n",
    "> S2: The movie was bad. It was not good. \n",
    "\n",
    "One of the primary advantages of an RNN is that it takes into account ordering of the words and sequential information available in text. That said, text classification models with bag-of-words representation are fast to implement and train and easy to understand, and in many cases they are good enough for the given task. So let's examine how well these models are performing on our task so that later we will be able to check whether we are gaining anything with RNNs or not.  \n",
    "\n",
    "1. Implement `DummyClassifier` baseline. Report cross-validation scores. \n",
    "2. Implement `LogisticRegression` with bag-of-words representation (`CountVectorizer`). Report cross-validation scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "Rw-LfI2uhh4E",
    "outputId": "c7d25be8-8f2e-4667-fb4a-8f578ab4e977"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.006146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.002984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.245202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.240482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dummy\n",
       "fit_time        0.006146\n",
       "score_time      0.002984\n",
       "test_accuracy   0.245202\n",
       "train_accuracy  0.240482"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X_train, y_train = big_train_df[\"moment\"], big_train_df[\"label\"]\n",
    "X_test, y_test = test_df[\"moment\"], test_df[\"label\"]\n",
    "scoring = [\"accuracy\"]\n",
    "results = {}\n",
    "dummy = DummyClassifier(strategy='stratified')\n",
    "scores = cross_validate(dummy, X_train, y_train, return_train_score=True, scoring = scoring)\n",
    "results[\"dummy\"] = pd.DataFrame(scores).mean()\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIk5tr3mhh4E",
    "outputId": "319f413a-47d5-47f0-aa2c-e56971ea6e4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "text_transformer = make_pipeline(CountVectorizer(max_features=20_000))\n",
    "\n",
    "pipe = make_pipeline(text_transformer, LogisticRegression(class_weight=\"balanced\"))\n",
    "scores = cross_validate(pipe, X_train, y_train, return_train_score=True, scoring=scoring)\n",
    "results[\"Logistic Regression\"] = pd.DataFrame(scores).mean()\n",
    "#warnings.filterwarnings(\"ignore\"s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "fojkl94wBAdF",
    "outputId": "6e50aeed-de1f-4758-dfcd-6404a09afa48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.006146</td>\n",
       "      <td>2.715815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.088651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.245202</td>\n",
       "      <td>0.817417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.240482</td>\n",
       "      <td>0.951031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dummy  Logistic Regression\n",
       "fit_time        0.006146  2.715815           \n",
       "score_time      0.002984  0.088651           \n",
       "test_accuracy   0.245202  0.817417           \n",
       "train_accuracy  0.240482  0.951031           "
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khOAGelLzuC_"
   },
   "source": [
    "## 2: Preparing the data for LSTMs <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "A lot of the work in developing neural network models for text data goes into data preprocessing and getting the text into the suitable format. There are a number of steps involved in data preprocessing. \n",
    "\n",
    "- [ ] Data splitting (train, valid, test)\n",
    "- [ ] Loading the data files\n",
    "- [ ] Tokenization\n",
    "- [ ] Creating a vocabulary: Creating a list of unique words \n",
    "- [ ] Numericalization: Converting text to a set of integers. \n",
    "- [ ] Word vectors\n",
    "- [ ] Embedding lookup \n",
    "- [ ] Batching\n",
    "\n",
    "We'll use the [`torchtext`](https://pypi.org/project/torchtext/) library to help us with some of these preprocessing steps. \n",
    "\n",
    "\n",
    "Install `torchtext` in your environment. \n",
    "\n",
    "```pip install torchtext```\n",
    "\n",
    "> This lab assumes that you have `torchtext` version 0.9.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1-ZJhn9Fhh4F"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.data import (\n",
    "    BucketIterator,\n",
    "    Field,\n",
    "    Iterator,\n",
    "    LabelField,\n",
    "    TabularDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mMP3sLWhh4F"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uueWSl33hh4F"
   },
   "source": [
    "### 2.1 Tokenization and defining TEXT and LABEL\n",
    "\n",
    "We'll use `spaCy` for tokenization and tell `PyTorch` how we want our text and label fields to be processed using `torchtext`'s [`Field`]((https://torchtext.readthedocs.io/en/latest/data.html?highlight=Field#field). The code below defines `TEXT` and `LABEL` using `torchtext`'s [`Field`](https://torchtext.readthedocs.io/en/latest/data.html?highlight=Field#field).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "19LfVfSZhh4F"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "\n",
    "def tokenize_spacy(text):\n",
    "    \"\"\"\n",
    "    return tokenized text using spacy\n",
    "    \"\"\"\n",
    "    # Solution_2_1_1\n",
    "    ### YOUR ANSWER HERE\n",
    "    return [word.text for word in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OviaC6euhh4G",
    "outputId": "a972a515-ca74-4209-ba32-f72fbe34c514"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', '!']"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_spacy(\"This is a test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JMyjscxFhh4G"
   },
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, tokenize=tokenize_spacy, lower=True)\n",
    "LABEL = Field(sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeyjUONxhh4G"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwmjCF25hh4G"
   },
   "source": [
    "### 2.2 Load data \n",
    "We need to tell the declared fields above what data they should work with. Let's load the data with [`TabularDataset`](https://torchtext.readthedocs.io/en/latest/data.html?highlight=TabularDataset#tabulardataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jUZbJrRSlH1d"
   },
   "outputs": [],
   "source": [
    "fields = [(\"moment\", TEXT), (\"label\", LABEL)]\n",
    "train, valid, test = TabularDataset.splits(\n",
    "    #path=\"./data/\",  # the root directory where the data lies\n",
    "    path=\"gdrive/MyDrive/data/\",  # the root directory where the data lies\n",
    "    train=\"train.csv\",\n",
    "    validation=\"valid.csv\",\n",
    "    test=\"test.csv\",\n",
    "    format=\"csv\",\n",
    "    skip_header=True,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WFwZHYNhh4H"
   },
   "source": [
    "> Here, we are retrieving the train, valid, and test data files that were created in Exercise 1.1. The columns to define those datasets are defined by `fields`. This is to allows us to perform tokenization of all the \"moment\" columns in all the files and preprocess given the labels from \"label\". The preprocess tabluar datasets are returned at the end, each in its own object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCHd0Z-3hh4H"
   },
   "source": [
    "### 2.3 Build and explore vocabulary\n",
    "The next step is building vocabularies for TEXT and LABEL so that we can convert text into integer sequences. The code below builds vocabulary for `TEXT` with `glove.6B.100d` vectors and vocabulary for LABEL (integers associated with unique labels). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAzb3baRhh4H",
    "outputId": "ea94d4fb-7fbf-4ead-a411-de9181d30bf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
      "100%|█████████▉| 398552/400000 [00:15<00:00, 26113.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab:  3464\n",
      "10 Most common words of vocab:  ['.', 'i', 'my', 'a', 'to', 'and', 'the', 'was', ',', 'for']\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, min_freq=3, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "print(\"Size of vocab: \", len(TEXT.vocab))\n",
    "top_10_words = [(coup[0]) for coup in TEXT.vocab.freqs.most_common(10)]\n",
    "print(\"10 Most common words of vocab: \", top_10_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sqft74Pnhh4H"
   },
   "outputs": [],
   "source": [
    "sents = [\n",
    "    \"my students will graduate soon!\",\n",
    "    \"we are building a preprocessing pipeline for lstms.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEfX7h1qhh4H",
    "outputId": "30eee82b-612e-4d0d-9450-4479c0d2a103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   4,   25],\n",
      "        [ 684,   97],\n",
      "        [ 153, 1005],\n",
      "        [ 874,    5],\n",
      "        [ 660,    0],\n",
      "        [  55,    0],\n",
      "        [   1,   11],\n",
      "        [   1,    0],\n",
      "        [   1,    2]])\n"
     ]
    }
   ],
   "source": [
    "sent_1 = TEXT.preprocess(sents[0])\n",
    "sent_2 = TEXT.preprocess(sents[1])\n",
    "\n",
    "tensor = TEXT.process([sent_1, sent_2])\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuHbj7c-hh4I"
   },
   "source": [
    "> There are 3 unknown words, represented by 0 in the numerical tensor representation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLCYw0c3hh4J"
   },
   "source": [
    "### 2.4 train, valid, test iterators  \n",
    "Now that we know how to numericalize data, the next step is padding and creating iterators for `train`, `valid`, and `test` splits. Below code\n",
    "- creates these iterators with `sort = False` and `sort_within_batch=False`  arguments \n",
    "- shows first few padded examples from the first batch \n",
    "\n",
    "Create iterators `train_iter`, `valid_iter`, and `test_iter` using `BucketIterator.splits` with `sort=True` and `sort_within_batch=True` and examine the first batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AH1QRLaXhh4J"
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(32, 32, 32),\n",
    "    sort_key=lambda x: len(x.moment),\n",
    "    sort=False,  # setting it to False to examine the batch.\n",
    "    sort_within_batch=False,  # setting it to False to examine the batch.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Oc5UXfZAhh4J"
   },
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    moments = batch.moment\n",
    "    labels = batch.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WLwB0jMhh4K",
    "outputId": "90bee1c9-005d-45de-83ca-535400229a4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "bZ3p8s57hh4K"
   },
   "outputs": [],
   "source": [
    "def print_preprocessed_examples(moments, labels, n=4):\n",
    "    print(\"preprocessed corpus:\")\n",
    "    df_data = defaultdict(list)\n",
    "    for j in range(n):  # sample loop\n",
    "        df_data[\"tokens\"].append(\n",
    "            [TEXT.vocab.itos[moments[i, j]] for i in range(moments.shape[0])]\n",
    "        )\n",
    "        df_data[\"example\"].append(j)\n",
    "        df_data[\"label\"] = labels[j].item()\n",
    "    return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "pYlUI3lthh4K",
    "outputId": "9ad284e4-9811-4fe1-be7f-5408fa79cebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed corpus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>example</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, met, my, sister, after, 15, years, of, not, seeing, each, other, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[at, the, afternoon, ,, when, i, review, the, money, earned, in, my, mturk, &lt;unk&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, painted, my, home, last, month, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[my, mother, came, to, visit, and, we, got, along, great, ., &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;, &lt;pad&gt;]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        tokens  ...  label\n",
       "0  [i, met, my, sister, after, 15, years, of, not, seeing, each, other, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]           ...  0    \n",
       "1  [at, the, afternoon, ,, when, i, review, the, money, earned, in, my, mturk, <unk>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]           ...  0    \n",
       "2  [i, painted, my, home, last, month, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]  ...  0    \n",
       "3  [my, mother, came, to, visit, and, we, got, along, great, ., <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>, <pad>]            ...  0    \n",
       "\n",
       "[4 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_preprocessed_examples(moments, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Pvqqjiawhh4K"
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_sizes=(32, 32, 32),\n",
    "    sort_key=lambda x: len(x.moment),\n",
    "    sort=True,  # setting it to True to examine the batch.\n",
    "    sort_within_batch=True,  # setting it to True to examine the batch.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qVXPgmTTIf-W"
   },
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    moments = batch.moment\n",
    "    labels = batch.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IRkagunyIkeX",
    "outputId": "2538993a-08d8-4530-c161-01c9b8cdda0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "R7vgCJxNI4NT",
    "outputId": "94850d2e-9c36-4007-d972-2cd714f2ae97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed corpus:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>example</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[enjoy, with, friends]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[enjoy, with, family]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[i, went, movie]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[night, shopping, event]</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tokens  example  label\n",
       "0  [enjoy, with, friends]    0        3    \n",
       "1  [enjoy, with, family]     1        3    \n",
       "2  [i, went, movie]          2        3    \n",
       "3  [night, shopping, event]  3        3    "
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_preprocessed_examples(moments, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cwx64tGBhh4L"
   },
   "source": [
    "> When using `sort=True` and `sort_within_bath=True`, we are grouping examples of similar lengths together to ensure we are minimizing the padding, which is why it is the preferred approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIFjet69hh4L"
   },
   "source": [
    "### 2.5 Embedding lookup\n",
    "\n",
    "Instead of one-hot representation of words, we will be feeding in 100-dimensional GloVe pre-trained embedding representation of words to the network.  \n",
    "\n",
    "The code below creates embedding lookup table for all words in our vocabulary and gets embeddings for the `moments` object from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1klnb_-Nhh4L",
    "outputId": "b9a170b8-8687-4895-9bd3-d701c1181058"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3464"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_VEC_SIZE = 100\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-lvrvH1hh4L",
    "outputId": "34cdb499-98b8-4566-de6f-11a84845c132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup table shape =  torch.Size([3464, 100])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(VOCAB_SIZE, WORD_VEC_SIZE, sparse=True)\n",
    "print(\"lookup table shape = \", embedding.weight.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "K5Jl564Ghh4L"
   },
   "outputs": [],
   "source": [
    "moments_embeddings = embedding(moments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edf8xPlNhh4L",
    "outputId": "d1af437b-0938-44c1-89aa-bade5e31e8d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 100])"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqnff-tEhh4M"
   },
   "source": [
    "> The shape of `moments_embeddings` is [3, 32, 100]. The 3 corresponds to the number of sentences, followed by the 32 words in each and embeddings representation of size 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fdm_PMC7hh4M",
    "outputId": "c26ad9f6-880e-4b73-f98c-c25ae3c62b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3702, -0.1596, -0.2809,  1.0452, -0.1587, -0.0889, -1.1730, -0.1050,\n",
       "         0.1134,  0.0509, -0.8071,  1.1902,  0.1789, -0.4104,  0.2496, -0.7975,\n",
       "         0.2241,  0.0599, -0.5650,  1.3956,  1.0549,  0.0928,  1.2104, -0.4676,\n",
       "         0.5980, -0.6787,  0.1325, -0.1865,  0.5388, -0.9643, -0.8258, -1.4535,\n",
       "         0.9183, -0.3783,  2.7459, -0.1653,  0.6637,  0.0303, -0.0471, -1.4707,\n",
       "        -0.2160,  1.6368, -1.1374,  0.5581, -0.7656,  1.0078,  0.7132, -0.6409,\n",
       "        -0.0882,  1.2613,  1.5378, -0.8134,  0.6688,  1.3540, -0.2638, -0.0259,\n",
       "        -0.0537, -0.7988, -0.6717,  0.8704, -1.3674, -0.3977, -0.0617,  1.1158,\n",
       "         1.2468, -1.7712, -1.5736, -0.4420, -1.6735,  0.7503,  0.2669, -1.2169,\n",
       "         0.6866,  0.8340,  0.6806, -1.3048,  0.2233,  2.5756, -0.0092,  0.5507,\n",
       "         1.1500, -0.5679, -0.1882,  0.2451,  0.2769, -1.7062,  0.6988, -0.8582,\n",
       "        -0.5770, -0.2300,  0.4017, -0.8018, -0.4263,  0.3483,  0.0632, -0.5791,\n",
       "         0.8394, -1.5801, -0.9189,  1.5481], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moments_embeddings[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_73e-iBrhh4M"
   },
   "source": [
    "> It is a good idea to pass pre-trained word embeddings to the network in our task since the pretrained embedding were training on a large corpus which we consider is reliable enough. Creating our own embeddings on a smaller corpus is likely to be reliable and accurate. Also, it is quite expensive and time consuming to train word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgU3vmi3hh4M"
   },
   "source": [
    "## 3: Defining LSTM network architecture <a name=\"3\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have preprocessed our data, we are now ready to define our network architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwEB0y8Uhh4M"
   },
   "source": [
    "### 3.1 LSTMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "vvXubcp-hh4N"
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_size, vocab_size, output_size, hidden_size, num_layers\n",
    "    ):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Solution_3_1_1\n",
    "        ### YOUR ANSWER HERE\n",
    "\n",
    "        # code adapted from demo\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embedding_size\n",
    "        )\n",
    "        \n",
    "        self.lstm_rnn = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.linear_layer = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax_layer = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Solution_3_1_1\n",
    "        ### YOUR ANSWER HERE\n",
    "        out = self.embedding(x)\n",
    "        out, (h_state, c_state) = self.lstm_rnn(\n",
    "            out\n",
    "        )\n",
    "        # classify based on the hidden representation at the last token\n",
    "        out = out[-1]\n",
    "        out = self.activation_fn(out)\n",
    "        out = self.linear_layer(out)\n",
    "        out = self.softmax_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x355PKzphh4N"
   },
   "source": [
    "> For my network architecture, I defined the following layers:\n",
    "> - Embedding layer (`nn.Embedding`)\n",
    ">   - This layer is responsible fort crearing lookup tables and initializing embeddings using inputs the same size of `TEXT` vocabulary. It can be used to obtain task-specific embeddings during the learning process.\n",
    "> - LSTM layers (`nn.LSTM`)\n",
    ">   - The number of LSTM layers is defined by `num_layers`.\n",
    ">   - The layer takes input size, hidden size, and number of layers as inputs\n",
    "> - An activation function layer for non linearity (`nn.Tanh`)\n",
    "> - A linear layer (`nn.Linear`)\n",
    "> - An output layer (`nn.LogSoftmax`)\n",
    ">   - This layer acts on the outout of `nn.Linear` on top of the output of linear layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19L_tbqOhh4N"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzhRieNmhh4O"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWdsi1kShh4O"
   },
   "source": [
    "### Exercise 4: Training and evaluation <a name=\"4\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have defined our network architecture, we are ready to train our model. Let's first set the device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "HKYk2Gvrhh4O"
   },
   "outputs": [],
   "source": [
    "manual_seed = 123\n",
    "torch.manual_seed(manual_seed)  # set the seed (for reproducibility)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XugFp64vhh4O"
   },
   "source": [
    "Given a data iterator, the `train` and `evaluate` functions below train and evaluate the model for all batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9h-iZvB-hh4O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    total_loss = 0.0\n",
    "    # iterate throught the data loader\n",
    "    num_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # load the current batch\n",
    "        batch_input = batch.moment\n",
    "        batch_output = batch.label\n",
    "\n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        model_outputs = model(batch_input)\n",
    "\n",
    "        # compute the loss\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.item()\n",
    "\n",
    "        # backward propagation (compute the gradients and update the model)\n",
    "        # clear the buffer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute the gradients\n",
    "        cur_loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        num_samples += batch_output.shape[0]\n",
    "\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Ej_hgecs0m9o"
   },
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    with torch.no_grad():  # for efficiency\n",
    "        for batch in dataloader:\n",
    "            # load the current batch\n",
    "            try:\n",
    "                batch_input = batch.moment\n",
    "                batch_output = batch.label\n",
    "                batch_input = batch_input.to(device)\n",
    "                batch_output = batch_output.to(device)\n",
    "                # forward propagation\n",
    "                model_outputs = model(batch_input)\n",
    "                # identify the predicted class for each example in the batch\n",
    "                probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "                preds.extend(predicted.cpu())\n",
    "                labels.extend(batch_output.cpu())\n",
    "            except:\n",
    "                print(\"Error calculating predictions\")\n",
    "                print(batch)\n",
    "    accuracy = accuracy_score(preds, labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUaapxezhh4P"
   },
   "source": [
    "### 4.1 Instantiating the model\n",
    "\n",
    "Define hyperparameters of the model, instantiate the model using `LSTMModel`, examine the number of parameters of the model, and create a `checkpoint` directory to save models after each epoch.  \n",
    "\n",
    "> For some pre-defined hyperparameters, I am using some standard values. Feel free to experiment with different values for the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8Bd9lLJBhh4P"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128  # number of units in the hidden layer\n",
    "NUM_LAYERS = 2  # number of hidden layers\n",
    "MAX_EPOCHS = 20  # number of passes over the training data\n",
    "LEARNING_RATE = 0.2  # learning rate for the weight update rule\n",
    "NUM_CLASSES = 7  # number of classes for the problem\n",
    "EMBEDDING_SIZE = 100  # size of the word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lal_NIzXR38P"
   },
   "outputs": [],
   "source": [
    "# Create a directory for writing models.\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = \"gdrive/MyDrive/checkpoint\"\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysae5jEZhh4P",
    "outputId": "d3364ba6-eb3c-47fb-8647-e0a6c326eec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (embedding): Embedding(3464, 100)\n",
      "  (lstm_rnn): LSTM(100, 128, num_layers=2)\n",
      "  (activation_fn): ReLU()\n",
      "  (linear_layer): Linear(in_features=128, out_features=7, bias=True)\n",
      "  (softmax_layer): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "\n",
    "model = LSTMModel(\n",
    "    EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "I5oSGQL2hh4P"
   },
   "outputs": [],
   "source": [
    "model.to(device)  # ship the  to the right device\n",
    "criterion = nn.NLLLoss()  # define the loss function (last node of the network)\n",
    "\n",
    "# Count the number of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FoSPEXA2hh4P",
    "outputId": "3dae0bc8-0185-4539-85e9-8fb1a43e7fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597159"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSbxM1C5hh4Q"
   },
   "source": [
    "> The embedding layer input dimension is equivalent to the total vocabulary in the text. The output is an embeddings representation of the embedding size, defined as 100. The dimension of the linear layer is equal to the number of classes, defined as 7. There are 2  unidirectional LSTM layers defined by num_layers whose input is defined by num_class, input_sice, and hidden_size. As input, it takes the embedding size and outputs a dimension of size hidden_size, which is then fed into the linear layer. The activation layers have the same dimensions, taking the last hidden layer as input before feeding it to the linear layer. We pass the last hidden layer into an activation function before feeding it into a linear layer. The output of the softmax layer will be the same dimension as the number of classes since the layer gives the log probabilities of each class. There are 597159 trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdW8ZSR-hh4Q"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8640MfUkhh4Q"
   },
   "source": [
    "### 4.2 Initializing the pretrained embeddings and carry out one forward pass\n",
    "\n",
    "The code below initializes the `model` above with pre-trained GloVe embeddings. \n",
    "\n",
    "Run the code below to carry out one forward pass over a sample input batch (`moments` and `labels`) from 2.4 above and report the loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTqxhjyKhh4Q",
    "outputId": "4fe2a153-68d6-455e-f04b-1acb6ad672c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3464, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "8ofLbsYtRgke"
   },
   "outputs": [],
   "source": [
    "moments = moments.to(device)\n",
    "labels = labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ko2Ah52eMw-c",
    "outputId": "09c0fbe3-27f0-4295-8057-ab820e967b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  1.9304802417755127\n"
     ]
    }
   ],
   "source": [
    "preds = model(moments)\n",
    "loss = criterion(preds, labels)\n",
    "print(\"loss = \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwEF8F6thh4Q"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5tt_fnVhh4Q"
   },
   "source": [
    "### 4.3 Train the model\n",
    "\n",
    "The code below trains the model for MAX_EPOCHS epochs.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ql7j6KKlhh4R",
    "outputId": "b0d0bd97-711e-4d64-9211-20f5e90c4013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.0483, Train accuracy 0.3430; Validation accuracy 0.3417\n",
      "Epoch 2, Loss 0.0403, Train accuracy 0.4457; Validation accuracy 0.4274\n",
      "Epoch 3, Loss 0.0327, Train accuracy 0.4814; Validation accuracy 0.4632\n",
      "Epoch 4, Loss 0.0291, Train accuracy 0.6238; Validation accuracy 0.6268\n",
      "Epoch 5, Loss 0.0266, Train accuracy 0.3424; Validation accuracy 0.3417\n",
      "Epoch 6, Loss 0.0224, Train accuracy 0.4690; Validation accuracy 0.4503\n",
      "Epoch 7, Loss 0.0193, Train accuracy 0.4400; Validation accuracy 0.4310\n",
      "Epoch 8, Loss 0.0176, Train accuracy 0.7350; Validation accuracy 0.7149\n",
      "Epoch 9, Loss 0.0166, Train accuracy 0.6761; Validation accuracy 0.6606\n",
      "Epoch 10, Loss 0.0159, Train accuracy 0.8041; Validation accuracy 0.7747\n",
      "Epoch 11, Loss 0.0153, Train accuracy 0.8079; Validation accuracy 0.7790\n",
      "Epoch 12, Loss 0.0145, Train accuracy 0.8131; Validation accuracy 0.7845\n",
      "Epoch 13, Loss 0.0135, Train accuracy 0.8201; Validation accuracy 0.7924\n",
      "Epoch 14, Loss 0.0125, Train accuracy 0.8661; Validation accuracy 0.8262\n",
      "Epoch 15, Loss 0.0119, Train accuracy 0.8680; Validation accuracy 0.8219\n",
      "Epoch 16, Loss 0.0115, Train accuracy 0.8794; Validation accuracy 0.8270\n",
      "Epoch 17, Loss 0.0107, Train accuracy 0.8878; Validation accuracy 0.8333\n",
      "Epoch 18, Loss 0.0101, Train accuracy 0.8894; Validation accuracy 0.8250\n",
      "Epoch 19, Loss 0.0096, Train accuracy 0.8972; Validation accuracy 0.8348\n",
      "Epoch 20, Loss 0.0092, Train accuracy 0.8538; Validation accuracy 0.8034\n"
     ]
    }
   ],
   "source": [
    "# create an instance of SGD with required hyperparameters\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "epochs, losses, train_accs, valid_accs = [], [], [], []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    # train the model for one pass over the data\n",
    "    train_loss = train(train_iter)\n",
    "    losses.append(train_loss)\n",
    "\n",
    "    # compute the training accuracy\n",
    "    train_acc = evaluate(train_iter)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # compute the validation accuracy\n",
    "    valid_acc = evaluate(valid_iter)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    epochs.append(epoch + 1)\n",
    "\n",
    "    # print the loss for every epoch\n",
    "    print(\n",
    "        \"Epoch %d, Loss %0.4f, Train accuracy %0.4f; Validation accuracy %0.4f\"\n",
    "        % (epoch + 1, train_loss, train_acc, valid_acc)\n",
    "    )\n",
    "\n",
    "    # save model, optimizer, and number of epoch to a dictionary\n",
    "    model_save = {\n",
    "        \"epoch\": epoch,  # number of epoch\n",
    "        \"model_state_dict\": model.state_dict(),  # model parameters\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),  # save optimizer\n",
    "        \"loss\": train_loss,  # training loss\n",
    "    }\n",
    "    torch.save(model_save, CHECKPOINT_PATH + \"/model_{}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "1c0KF4xBhh4R",
    "outputId": "b0a9f738-f0d8-4888-b517-6a0aad53a8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train Accuracy Cuve')"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epochs, train_accs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train Accuracy Cuve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "dh3tWdJRF6cs",
    "outputId": "5ece0c2d-fa5f-43b7-eae0-149747b8138c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Accuracy Cuve')"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, valid_accs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy Cuve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "HIbIrbswGI4h",
    "outputId": "c1e80a19-292c-45d5-d396-d6d60f69c989"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.006146</td>\n",
       "      <td>2.715815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.002984</td>\n",
       "      <td>0.088651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.245202</td>\n",
       "      <td>0.817417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.240482</td>\n",
       "      <td>0.951031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   dummy  Logistic Regression\n",
       "fit_time        0.006146  2.715815           \n",
       "score_time      0.002984  0.088651           \n",
       "test_accuracy   0.245202  0.817417           \n",
       "train_accuracy  0.240482  0.951031           "
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aindDGIKhh4R"
   },
   "source": [
    "> We see that on average with each additional epoch, the train and validation accuracies are improving. Comparing the above accuracy curves to the baseline models from exercise 1, we see that for each epoch, the gap between the validation scores and the training scores are much smaller, indicating less overfitting by the LTSM model. Naturally it performed better than the dummy classifier. The logistic regression model seems to do the most overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WqvMqbchh4R"
   },
   "source": [
    "### 4.4 Evaluation on the test data \n",
    "\n",
    "1. Get test accuracy with the final model. \n",
    "2. Compare results with train and validation accuracies. Do you see overfitting? In our previous machine learning courses we usually used cross-validation. What might be the reason of using a single validation set here?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JsZKjNQhh4R",
    "outputId": "de18ba49-5c74-4d3c-abc0-71575ea13be3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7968860580325549"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMmx1zNQhh4R"
   },
   "source": [
    "> There seems to be a little bit of overfitting here as the training accuracy of around 0.89 is larger than the test accuracy of 0.79, though both scores are fairly comparable. The use of a single validation set here is due to avoid overfitting which would occur with multiple splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xrb4-Ksshh4S"
   },
   "source": [
    "### 4.5 Evaluation on unseen examples\n",
    "\n",
    "The code below loads one of the saved models. Let's try it out on unseen examples.  \n",
    "\n",
    "We shall examine predictions given by the model on `test_sents` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwVa8oeuhh4S",
    "outputId": "127e91a2-b44e-4bfd-db98-936804782060"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embedding): Embedding(3464, 100)\n",
       "  (lstm_rnn): LSTM(100, 128, num_layers=2)\n",
       "  (activation_fn): ReLU()\n",
       "  (linear_layer): Linear(in_features=128, out_features=7, bias=True)\n",
       "  (softmax_layer): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model2 = LSTMModel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS)\n",
    "path = CHECKPOINT_PATH + \"/model_19.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "model2.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gm_VLaNFhh4S",
    "outputId": "9750df60-475c-429f-e26f-0263d77bec9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affection',\n",
       " 'achievement',\n",
       " 'bonding',\n",
       " 'enjoy_the_moment',\n",
       " 'leisure',\n",
       " 'nature',\n",
       " 'exercise']"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict = [tup[0] for tup in LABEL.vocab.freqs.most_common()]\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "EP0U-8uGdz9x"
   },
   "outputs": [],
   "source": [
    "topic_labels = {0: 'affection',\n",
    "                1:'achievement',\n",
    "                2:'bonding',\n",
    "                3:'enjoy_the_moment',\n",
    "                4:'leisure',\n",
    "                5:'nature',\n",
    "                6:'exercise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "99YnoLqFH1uC"
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "KzvbJhILhh4S"
   },
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    \"\"\"\n",
    "    Given model and text, return predicted label for the happy moment text.\n",
    "    \"\"\"\n",
    "    for sent in test_sents:\n",
    "        atensor =  TEXT.process([TEXT.preprocess(sent)])\n",
    "        atensor = atensor.to(device)\n",
    "        pred = model(atensor)\n",
    "        max_prob = torch.argmax(pred)\n",
    "        make_me = max_prob.tolist()\n",
    "        results[sent] = make_me \n",
    "        \n",
    "    tmp = pd.DataFrame.from_dict(results, orient='index', columns=['Predicted Label'])\n",
    "    tmp['Prediction'] = tmp['Predicted Label'].map(topic_labels)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "qjWd2IAbhh4S",
    "outputId": "29e29aab-cc01-4c6d-dd84-c118d197e692"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Moment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just finished my last assignment!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On the weekend, I spent some quality time with my best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I told my students that I love them all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I went for a hike in the forest.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I did yoga this morning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I am still breathing and I am alive!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Moment\n",
       "0  I just finished my last assignment!                           \n",
       "1  On the weekend, I spent some quality time with my best friend.\n",
       "2  I told my students that I love them all.                      \n",
       "3  I went for a hike in the forest.                              \n",
       "4  I did yoga this morning.                                      \n",
       "5  I am still breathing and I am alive!                          "
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents = [\n",
    "    \"I just finished my last assignment!\",\n",
    "    \"On the weekend, I spent some quality time with my best friend.\",\n",
    "    \"I told my students that I love them all.\",\n",
    "    \"I went for a hike in the forest.\",\n",
    "    \"I did yoga this morning.\",\n",
    "    \"I am still breathing and I am alive!\",\n",
    "]\n",
    "pred_df = pd.DataFrame(test_sents, columns=[\"Moment\"])\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "AaQ4io6ehh4S",
    "outputId": "aa7eb445-40e4-46d2-f24d-4ea892c53cb8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Label</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I just finished my last assignment!</th>\n",
       "      <td>1</td>\n",
       "      <td>achievement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>On the weekend, I spent some quality time with my best friend.</th>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I told my students that I love them all.</th>\n",
       "      <td>2</td>\n",
       "      <td>bonding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I went for a hike in the forest.</th>\n",
       "      <td>5</td>\n",
       "      <td>nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I did yoga this morning.</th>\n",
       "      <td>6</td>\n",
       "      <td>exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am still breathing and I am alive!</th>\n",
       "      <td>1</td>\n",
       "      <td>achievement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Predicted Label   Prediction\n",
       "I just finished my last assignment!                             1                achievement\n",
       "On the weekend, I spent some quality time with my best friend.  2                bonding    \n",
       "I told my students that I love them all.                        2                bonding    \n",
       "I went for a hike in the forest.                                5                nature     \n",
       "I did yoga this morning.                                        6                exercise   \n",
       "I am still breathing and I am alive!                            1                achievement"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model2, test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekrI0SjBhh4S"
   },
   "source": [
    "<br><br>\n",
    "\n",
    "> The predictions made by the LSTM model seem to be reasonable. All the labels make sense! For instance, the label *bonding* is assigned to the text describing bonding time between friends. However, 3 out of the 6 sentences were assigned the label *achievement*. Further investigating those sentences, we see that perhaps the one about yoga could be labeled *exercise*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEku-6snhh4T"
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek4q7-Wyhh4T"
   },
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
